import os
from argparse import ArgumentParser

import torch
import torch.nn.functional as F
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import transforms

from datasets.fashioniq_dataset import FashionDataset

# Gi·∫£ s·ª≠ b·∫°n ƒë√£ ƒë·ªãnh nghƒ©a m√¥ h√¨nh FashionVLP theo ki·∫øn tr√∫c trong b√†i b√°o
# G·ªìm 2 nh√°nh: Reference block v√† Target block.
# class FashionVLP(nn.Module): ...
from modules.fashion_vlp import (
    FashionVLP,  # T∆∞·ªüng t∆∞·ª£ng c√≥ file model.py ƒë·ªãnh nghƒ©a l·ªõp n√†y
)

parser = ArgumentParser()
parser.add_argument("--batch", default=64, type=int)
parser.add_argument("--path-file", required=True, type=str)
parser.add_argument("--path-folder", required=True, type=str)
args = parser.parse_args()


# H√†m m·∫•t m√°t: Batch-based classification loss, v·ªÅ b·∫£n ch·∫•t l√† CrossEntropyLoss
# tr√™n ma tr·∫≠n t∆∞∆°ng ƒë·ªìng cosine[cite: 211, 214].
# N·ªôi nƒÉng (inner product) ƒë∆∞·ª£c d√πng l√†m h√†m kernel, t∆∞∆°ng ƒë∆∞∆°ng cosine similarity
# khi c√°c vector ƒë∆∞·ª£c chu·∫©n h√≥a[cite: 216].
def batch_classification_loss(f_ref, f_tar, temperature=0.07):
    """
    Tri·ªÉn khai h√†m loss t·ª´ ph∆∞∆°ng tr√¨nh (6) trong b√†i b√°o.
    ƒê√¢y l√† loss t∆∞∆°ng ph·∫£n (contrastive loss) InfoNCE.
    """
    # f_ref: [B, D], f_tar: [B, D]  (B: batch size, D: s·ªë chi·ªÅu embedding)
    B, D = f_ref.shape

    # Chu·∫©n h√≥a L2 c√°c embedding ƒë·ªÉ t√≠nh cosine similarity b·∫±ng t√≠ch v√¥ h∆∞·ªõng
    f_ref = F.normalize(f_ref, p=2, dim=1)
    f_tar = F.normalize(f_tar, p=2, dim=1)

    # T√≠nh ma tr·∫≠n t∆∞∆°ng ƒë·ªìng (similarity matrix)
    # T√≠ch v√¥ h∆∞·ªõng gi·ªØa m·ªói f_ref v√† T·∫§T C·∫¢ c√°c f_tar trong batch
    # sim_matrix[i, j] = cos_sim(f_ref_i, f_tar_j)
    sim_matrix = torch.matmul(f_ref, f_tar.t()) / temperature  # -> [B, B]

    # Loss: cross-entropy gi·ªØa sim_matrix v√† ma tr·∫≠n ƒë∆°n v·ªã
    # C√°c c·∫∑p d∆∞∆°ng (positive pairs) n·∫±m tr√™n ƒë∆∞·ªùng ch√©o ch√≠nh (i, i)
    labels = torch.arange(B, device=f_ref.device)
    loss = F.cross_entropy(sim_matrix, labels)

    return loss


def calculate_recall_at_k(f_ref, f_tar, k=10):
    """
    T√≠nh Recall@K (R@K) cho vi·ªác ƒë√°nh gi√° hi·ªáu su·∫•t recommendation.

    Args:
        f_ref: tensor [B, D] - embeddings c·ªßa reference images
        f_tar: tensor [B, D] - embeddings c·ªßa target images
        k: int - s·ªë l∆∞·ª£ng top candidates ƒë·ªÉ t√≠nh recall

    Returns:
        recall_at_k: float - gi√° tr·ªã R@K
    """
    # Chu·∫©n h√≥a L2 c√°c embeddings
    f_ref = F.normalize(f_ref, p=2, dim=1)
    f_tar = F.normalize(f_tar, p=2, dim=1)

    # T√≠nh ma tr·∫≠n t∆∞∆°ng ƒë·ªìng cosine
    similarity_matrix = torch.matmul(f_ref, f_tar.t())  # [B, B]

    batch_size = similarity_matrix.size(0)
    correct_predictions = 0

    for i in range(batch_size):
        # L·∫•y ƒë·ªô t∆∞∆°ng ƒë·ªìng c·ªßa sample i v·ªõi t·∫•t c·∫£ target images
        similarities = similarity_matrix[i]

        # S·∫Øp x·∫øp theo th·ª© t·ª± gi·∫£m d·∫ßn v√† l·∫•y top-k indices
        _, top_k_indices = torch.topk(similarities, k, largest=True)

        # Ki·ªÉm tra xem ground truth (index i) c√≥ trong top-k kh√¥ng
        if i in top_k_indices:
            correct_predictions += 1

    recall_at_k = correct_predictions / batch_size
    return recall_at_k


def evaluate_model(model, val_loader, device, k=10):
    """
    ƒê√°nh gi√° m√¥ h√¨nh tr√™n validation set v·ªõi metric R@K.

    Args:
        model: m√¥ h√¨nh FashionVLP
        val_loader: DataLoader cho validation set
        device: thi·∫øt b·ªã t√≠nh to√°n (CPU/GPU)
        k: int - s·ªë l∆∞·ª£ng top candidates

    Returns:
        avg_recall_at_k: float - R@K trung b√¨nh tr√™n to√†n b·ªô validation set
    """
    model.eval()
    total_recall = 0
    num_batches = 0

    with torch.no_grad():
        for batch in val_loader:
            # Chuy·ªÉn d·ªØ li·ªáu l√™n GPU
            ref_images = batch["reference_image"].to(device)
            target_images = batch["target_image"].to(device)
            feedback_tokens = batch["feedback_tokens"].to(device)
            crop_reference_images = batch["crop_reference_image"].to(device)
            crop_target_images = batch["crop_target_image"].to(device)
            landmark_locations = batch["landmarks"]

            # Forward pass
            f_ref, f_tar = model(
                ref_images,
                feedback_tokens,
                target_images,
                crop_reference_images,
                crop_target_images,
                landmark_locations,
            )

            # T√≠nh R@K cho batch n√†y
            batch_recall = calculate_recall_at_k(f_ref, f_tar, k)
            total_recall += batch_recall
            num_batches += 1

    avg_recall_at_k = total_recall / num_batches if num_batches > 0 else 0
    return avg_recall_at_k


def load_model_checkpoint(checkpoint_path, model, optimizer=None):
    """
    Load model checkpoint t·ª´ file .pt

    Args:
        checkpoint_path: ƒë∆∞·ªùng d·∫´n ƒë·∫øn file checkpoint
        model: instance c·ªßa model c·∫ßn load weights
        optimizer: optimizer (optional) ƒë·ªÉ ti·∫øp t·ª•c training

    Returns:
        dict ch·ª©a th√¥ng tin checkpoint
    """
    checkpoint = torch.load(checkpoint_path, map_location="cpu")
    model.load_state_dict(checkpoint["model_state_dict"])

    if optimizer is not None and "optimizer_state_dict" in checkpoint:
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])

    print(f"ƒê√£ load model t·ª´ checkpoint: {checkpoint_path}")
    print(f"  Epoch: {checkpoint.get('epoch', 'N/A')}")
    print(f"  Loss: {checkpoint.get('loss', 'N/A')}")
    print(f"  Recall@10: {checkpoint.get('recall_at_10', 'N/A')}")

    return checkpoint


image_transform = transforms.Compose(
    [
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
    ]
)

train_dataset = FashionDataset(
    annotations_folder=args.path_file,
    folder_img=args.path_folder,
    transform=image_transform,
    type="train",
)
train_loader = DataLoader(train_dataset, batch_size=args.batch, shuffle=True)

val_dataset = FashionDataset(
    annotations_folder=args.path_file,
    folder_img=args.path_folder,
    transform=image_transform,
    type="val",
)
val_loader = DataLoader(val_dataset, batch_size=args.batch, shuffle=False)


device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model = FashionVLP().to(device)

optimizer = optim.Adam(model.parameters(), lr=4e-4, betas=(0.55, 0.999))


num_epochs = 100
validation_frequency = 5

# Th√™m bi·∫øn ƒë·ªÉ track best model
best_recall = 0.0
best_model_path = None

train_losses = []
val_recalls = []

for epoch in range(num_epochs):
    model.train()
    total_loss = 0
    num_batches = 0

    for batch in train_loader:
        ref_images = batch["reference_image"].to(device)
        target_images = batch["target_image"].to(device)
        feedback_tokens = batch["feedback_tokens"].to(device)
        crop_reference_images = batch["crop_reference_image"].to(device)
        crop_target_images = batch["crop_target_image"].to(device)
        landmark_locations = batch["landmarks"]

        optimizer.zero_grad()

        f_ref, f_tar = model(
            ref_images,
            feedback_tokens,
            target_images,
            crop_reference_images,
            crop_target_images,
            landmark_locations,
        )

        loss = batch_classification_loss(f_ref, f_tar)

        loss.backward()
        optimizer.step()

        total_loss += loss.item()
        num_batches += 1

    avg_loss = total_loss / num_batches
    train_losses.append(avg_loss)

    if (epoch + 1) % validation_frequency == 0 or epoch == 0:
        print(f"\n--- ƒê√°nh gi√° Validation t·∫°i Epoch {epoch + 1} ---")

        val_recall_10 = evaluate_model(model, val_loader, device, k=10)
        val_recalls.append(val_recall_10)

        print(f"Epoch [{epoch + 1}/{num_epochs}]")
        print(f"  Training Loss: {avg_loss:.4f}")
        print(f"  Validation R@10: {val_recall_10:.4f}")
        print("-" * 50)

        # L∆∞u model checkpoint ƒë·ªãnh k·ª≥
        os.makedirs("checkpoints", exist_ok=True)
        checkpoint_path = (
            f"checkpoints/model_epoch_{epoch + 1}_recall_{val_recall_10:.4f}.pt"
        )
        torch.save(
            {
                "epoch": epoch + 1,
                "model_state_dict": model.state_dict(),
                "optimizer_state_dict": optimizer.state_dict(),
                "loss": avg_loss,
                "recall_at_10": val_recall_10,
                "train_losses": train_losses,
                "val_recalls": val_recalls,
            },
            checkpoint_path,
        )
        print(f"Model checkpoint ƒë√£ ƒë∆∞·ª£c l∆∞u: {checkpoint_path}")

        # L∆∞u best model n·∫øu recall t·ªët h∆°n
        if val_recall_10 > best_recall:
            best_recall = val_recall_10
            best_model_path = f"checkpoints/best_model_recall_{best_recall:.4f}.pt"
            torch.save(
                {
                    "epoch": epoch + 1,
                    "model_state_dict": model.state_dict(),
                    "optimizer_state_dict": optimizer.state_dict(),
                    "loss": avg_loss,
                    "recall_at_10": val_recall_10,
                    "train_losses": train_losses,
                    "val_recalls": val_recalls,
                },
                best_model_path,
            )
            print(f"üéâ Best model ƒë∆∞·ª£c c·∫≠p nh·∫≠t: {best_model_path}")

    else:
        print(f"Epoch [{epoch + 1}/{num_epochs}], Training Loss: {avg_loss:.4f}")

print("\n" + "=" * 60)
print("HO√ÄN TH√ÄNH QU√ÅTR√åNH TRAINING")
print("=" * 60)
final_recall_10 = evaluate_model(model, val_loader, device, k=10)
print(f"K·∫øt qu·∫£ cu·ªëi c√πng - Validation R@10: {final_recall_10:.4f}")
if val_recalls:
    best_recall_training = max(val_recalls)
    print(f"R@10 t·ªët nh·∫•t trong qu√° tr√¨nh training: {best_recall_training:.4f}")

print(f"Training Loss cu·ªëi c√πng: {train_losses[-1]:.4f}")

# L∆∞u model cu·ªëi c√πng
os.makedirs("checkpoints", exist_ok=True)
final_model_path = (
    f"checkpoints/final_model_epoch_{num_epochs}_recall_{final_recall_10:.4f}.pt"
)
torch.save(
    {
        "epoch": num_epochs,
        "model_state_dict": model.state_dict(),
        "optimizer_state_dict": optimizer.state_dict(),
        "final_loss": train_losses[-1],
        "final_recall_at_10": final_recall_10,
        "train_losses": train_losses,
        "val_recalls": val_recalls,
        "best_recall": best_recall if val_recalls else final_recall_10,
    },
    final_model_path,
)
print(f"Final model ƒë√£ ƒë∆∞·ª£c l∆∞u: {final_model_path}")

# L∆∞u ch·ªâ model weights (nh·∫π h∆°n) ƒë·ªÉ d·ªÖ load
model_weights_path = "checkpoints/fashion_vlp_weights.pt"
torch.save(model.state_dict(), model_weights_path)
print(f"Model weights ƒë√£ ƒë∆∞·ª£c l∆∞u: {model_weights_path}")

if best_model_path:
    print(f"Best model ƒë√£ l∆∞u t·∫°i: {best_model_path}")

print("=" * 60)
